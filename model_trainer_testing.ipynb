{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "153a0114-94c8-4604-a028-86a7bb722609",
   "metadata": {},
   "source": [
    "# Model Training (TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e56dc83-85ce-4975-802b-89fbb0ec0273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for adding features to the auto model trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430374f4-d2f7-493c-8da6-f8abaace70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "    \n",
    "default_args = {\n",
    "    'learning_rate' : 5e-5,\n",
    "    'epochs' : 150,\n",
    "    'batch_size' : 128,\n",
    "    'model' : 'xlm-roberta-base',\n",
    "    'exp_desc' : 'default_run'\n",
    "}\n",
    "\n",
    "sys.argv = [\"script_name\", \"--learning_rate\", str(default_args[\"learning_rate\"]) ,\"--epochs\", str(default_args[\"epochs\"]),\"--batch_size\", str(default_args[\"batch_size\"]),\"--model\" , default_args[\"model\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908ef555-154d-491c-bd39-d7befd43d258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 23:04:48.961009: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-19 23:04:49.029967: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-19 23:04:49.030044: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-19 23:04:49.033621: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-19 23:04:49.053486: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-19 23:04:50.239439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 5e-05, Epochs: 150, Batch Size: 128 Model: xlm-roberta-base Exp Des: None\n",
      "||--------------------------------------||\n",
      "||===>> Starting run: XLM-RoBERTa__lr5e-05_ep150_bs128 with hyperparameters: {'learning_rate': 5e-05, 'epochs': 150, 'batch_size': 128}\n",
      "||--------------------------------------||\n",
      "TensorFlow version: 2.15.0\n",
      "GPUs detected:\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 23:04:55.425146: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 23:04:55.559989: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 23:04:55.560062: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 23:04:58.282958: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 23:04:58.283216: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 23:04:58.283258: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 23:04:58.497700: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 23:04:58.497882: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 23:04:58.497892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-04-19 23:04:58.497912: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:236] Using CUDA malloc Async allocator for GPU: 0\n",
      "2025-04-19 23:04:58.498652: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-19 23:04:58.498861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1753 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7fef747e9480> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x7fef747e9480> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 23:06:07.616143: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fee38992ce0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-19 23:06:07.616222: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Ti Laptop GPU, Compute Capability 8.6\n",
      "2025-04-19 23:06:07.660748: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-04-19 23:06:07.739406: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745075167.860961    1496 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     35/Unknown - 85s 2s/step - loss: 0.6817 - accuracy: 0.5829 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 23:06:59.563797: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3706380515799544397\n",
      "2025-04-19 23:06:59.563887: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 17546837108008072627\n",
      "2025-04-19 23:06:59.563900: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 10017999739242499569\n",
      "2025-04-19 23:07:18.790863: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 5666433411951032629\n",
      "2025-04-19 23:07:18.790943: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 11336024482607086437\n",
      "2025-04-19 23:07:18.790956: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 10017999739242499569\n",
      "2025-04-19 23:07:18.790963: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 17546837108008072627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.67042, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 124s 3s/step - loss: 0.6817 - accuracy: 0.5829 - val_loss: 0.6704 - val_accuracy: 0.8135\n",
      "Epoch 2/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6517 - accuracy: 0.7032  \n",
      "Epoch 2: val_loss improved from 0.67042 to 0.64994, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 82s 2s/step - loss: 0.6517 - accuracy: 0.7032 - val_loss: 0.6499 - val_accuracy: 0.8135\n",
      "Epoch 3/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6269 - accuracy: 0.7521  \n",
      "Epoch 3: val_loss improved from 0.64994 to 0.62649, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 98s 3s/step - loss: 0.6269 - accuracy: 0.7521 - val_loss: 0.6265 - val_accuracy: 0.8216\n",
      "Epoch 4/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6050 - accuracy: 0.7832 \n",
      "Epoch 4: val_loss improved from 0.62649 to 0.60472, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.6050 - accuracy: 0.7832 - val_loss: 0.6047 - val_accuracy: 0.8162\n",
      "Epoch 5/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5815 - accuracy: 0.7801 \n",
      "Epoch 5: val_loss improved from 0.60472 to 0.58291, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.5815 - accuracy: 0.7801 - val_loss: 0.5829 - val_accuracy: 0.8180\n",
      "Epoch 6/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5641 - accuracy: 0.7920  \n",
      "Epoch 6: val_loss improved from 0.58291 to 0.56425, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.5641 - accuracy: 0.7920 - val_loss: 0.5642 - val_accuracy: 0.8324\n",
      "Epoch 7/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5431 - accuracy: 0.8024  \n",
      "Epoch 7: val_loss improved from 0.56425 to 0.53939, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.5431 - accuracy: 0.8024 - val_loss: 0.5394 - val_accuracy: 0.8306\n",
      "Epoch 8/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5242 - accuracy: 0.8170 \n",
      "Epoch 8: val_loss improved from 0.53939 to 0.52069, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.5242 - accuracy: 0.8170 - val_loss: 0.5207 - val_accuracy: 0.8261\n",
      "Epoch 9/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5061 - accuracy: 0.8231  \n",
      "Epoch 9: val_loss improved from 0.52069 to 0.49735, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.5061 - accuracy: 0.8231 - val_loss: 0.4973 - val_accuracy: 0.8559\n",
      "Epoch 10/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4903 - accuracy: 0.8265 \n",
      "Epoch 10: val_loss improved from 0.49735 to 0.47696, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 69s 2s/step - loss: 0.4903 - accuracy: 0.8265 - val_loss: 0.4770 - val_accuracy: 0.8631\n",
      "Epoch 11/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4676 - accuracy: 0.8326 \n",
      "Epoch 11: val_loss improved from 0.47696 to 0.45755, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 69s 2s/step - loss: 0.4676 - accuracy: 0.8326 - val_loss: 0.4576 - val_accuracy: 0.8685\n",
      "Epoch 12/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4620 - accuracy: 0.8393 \n",
      "Epoch 12: val_loss improved from 0.45755 to 0.44208, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.4620 - accuracy: 0.8393 - val_loss: 0.4421 - val_accuracy: 0.8721\n",
      "Epoch 13/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4508 - accuracy: 0.8486  \n",
      "Epoch 13: val_loss improved from 0.44208 to 0.42444, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.4508 - accuracy: 0.8486 - val_loss: 0.4244 - val_accuracy: 0.8766\n",
      "Epoch 14/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4354 - accuracy: 0.8517  \n",
      "Epoch 14: val_loss improved from 0.42444 to 0.40900, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 69s 2s/step - loss: 0.4354 - accuracy: 0.8517 - val_loss: 0.4090 - val_accuracy: 0.8865\n",
      "Epoch 15/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4206 - accuracy: 0.8605 \n",
      "Epoch 15: val_loss improved from 0.40900 to 0.39464, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 69s 2s/step - loss: 0.4206 - accuracy: 0.8605 - val_loss: 0.3946 - val_accuracy: 0.8874\n",
      "Epoch 16/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4124 - accuracy: 0.8592 \n",
      "Epoch 16: val_loss improved from 0.39464 to 0.38185, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 84s 2s/step - loss: 0.4124 - accuracy: 0.8592 - val_loss: 0.3818 - val_accuracy: 0.8901\n",
      "Epoch 17/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3978 - accuracy: 0.8689  \n",
      "Epoch 17: val_loss improved from 0.38185 to 0.36875, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 69s 2s/step - loss: 0.3978 - accuracy: 0.8689 - val_loss: 0.3688 - val_accuracy: 0.8946\n",
      "Epoch 18/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3904 - accuracy: 0.8758 \n",
      "Epoch 18: val_loss improved from 0.36875 to 0.35672, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.3904 - accuracy: 0.8758 - val_loss: 0.3567 - val_accuracy: 0.8991\n",
      "Epoch 19/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3773 - accuracy: 0.8754  \n",
      "Epoch 19: val_loss improved from 0.35672 to 0.34526, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 85s 2s/step - loss: 0.3773 - accuracy: 0.8754 - val_loss: 0.3453 - val_accuracy: 0.8991\n",
      "Epoch 20/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3684 - accuracy: 0.8745  \n",
      "Epoch 20: val_loss improved from 0.34526 to 0.33606, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.3684 - accuracy: 0.8745 - val_loss: 0.3361 - val_accuracy: 0.8973\n",
      "Epoch 21/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3605 - accuracy: 0.8808  \n",
      "Epoch 21: val_loss improved from 0.33606 to 0.32426, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.3605 - accuracy: 0.8808 - val_loss: 0.3243 - val_accuracy: 0.9027\n",
      "Epoch 22/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3524 - accuracy: 0.8867  \n",
      "Epoch 22: val_loss improved from 0.32426 to 0.31515, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.3524 - accuracy: 0.8867 - val_loss: 0.3152 - val_accuracy: 0.9054\n",
      "Epoch 23/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3431 - accuracy: 0.8860 \n",
      "Epoch 23: val_loss improved from 0.31515 to 0.30722, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.3431 - accuracy: 0.8860 - val_loss: 0.3072 - val_accuracy: 0.9018\n",
      "Epoch 24/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3359 - accuracy: 0.8903  \n",
      "Epoch 24: val_loss improved from 0.30722 to 0.29766, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.3359 - accuracy: 0.8903 - val_loss: 0.2977 - val_accuracy: 0.9072\n",
      "Epoch 25/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3358 - accuracy: 0.8889  \n",
      "Epoch 25: val_loss improved from 0.29766 to 0.29190, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.3358 - accuracy: 0.8889 - val_loss: 0.2919 - val_accuracy: 0.9099\n",
      "Epoch 26/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3230 - accuracy: 0.8909 \n",
      "Epoch 26: val_loss improved from 0.29190 to 0.28531, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.3230 - accuracy: 0.8909 - val_loss: 0.2853 - val_accuracy: 0.9135\n",
      "Epoch 27/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3258 - accuracy: 0.8907 \n",
      "Epoch 27: val_loss improved from 0.28531 to 0.27746, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.3258 - accuracy: 0.8907 - val_loss: 0.2775 - val_accuracy: 0.9144\n",
      "Epoch 28/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3097 - accuracy: 0.8930 \n",
      "Epoch 28: val_loss did not improve from 0.27746\n",
      "35/35 [==============================] - 69s 2s/step - loss: 0.3097 - accuracy: 0.8930 - val_loss: 0.2827 - val_accuracy: 0.9027\n",
      "Epoch 29/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3129 - accuracy: 0.8973 \n",
      "Epoch 29: val_loss improved from 0.27746 to 0.26610, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.3129 - accuracy: 0.8973 - val_loss: 0.2661 - val_accuracy: 0.9135\n",
      "Epoch 30/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2992 - accuracy: 0.8963 \n",
      "Epoch 30: val_loss improved from 0.26610 to 0.26286, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.2992 - accuracy: 0.8963 - val_loss: 0.2629 - val_accuracy: 0.9144\n",
      "Epoch 31/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3048 - accuracy: 0.8975  \n",
      "Epoch 31: val_loss improved from 0.26286 to 0.25524, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.3048 - accuracy: 0.8975 - val_loss: 0.2552 - val_accuracy: 0.9198\n",
      "Epoch 32/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.8993 \n",
      "Epoch 32: val_loss did not improve from 0.25524\n",
      "35/35 [==============================] - 67s 2s/step - loss: 0.2863 - accuracy: 0.8993 - val_loss: 0.2558 - val_accuracy: 0.9180\n",
      "Epoch 33/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2953 - accuracy: 0.9063 \n",
      "Epoch 33: val_loss improved from 0.25524 to 0.24759, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.2953 - accuracy: 0.9063 - val_loss: 0.2476 - val_accuracy: 0.9225\n",
      "Epoch 34/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3022 - accuracy: 0.8963 \n",
      "Epoch 34: val_loss improved from 0.24759 to 0.24544, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 70s 2s/step - loss: 0.3022 - accuracy: 0.8963 - val_loss: 0.2454 - val_accuracy: 0.9207\n",
      "Epoch 35/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.9011 \n",
      "Epoch 35: val_loss improved from 0.24544 to 0.23923, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 103s 3s/step - loss: 0.2841 - accuracy: 0.9011 - val_loss: 0.2392 - val_accuracy: 0.9279\n",
      "Epoch 36/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2943 - accuracy: 0.8997  \n",
      "Epoch 36: val_loss did not improve from 0.23923\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2943 - accuracy: 0.8997 - val_loss: 0.2467 - val_accuracy: 0.9198\n",
      "Epoch 37/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.9038 \n",
      "Epoch 37: val_loss improved from 0.23923 to 0.23872, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.2883 - accuracy: 0.9038 - val_loss: 0.2387 - val_accuracy: 0.9189\n",
      "Epoch 38/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.9058 \n",
      "Epoch 38: val_loss improved from 0.23872 to 0.23205, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 69s 2s/step - loss: 0.2709 - accuracy: 0.9058 - val_loss: 0.2321 - val_accuracy: 0.9288\n",
      "Epoch 39/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2821 - accuracy: 0.9013 \n",
      "Epoch 39: val_loss improved from 0.23205 to 0.22682, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.2821 - accuracy: 0.9013 - val_loss: 0.2268 - val_accuracy: 0.9315\n",
      "Epoch 40/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.9004  \n",
      "Epoch 40: val_loss did not improve from 0.22682\n",
      "35/35 [==============================] - 68s 2s/step - loss: 0.2883 - accuracy: 0.9004 - val_loss: 0.2276 - val_accuracy: 0.9306\n",
      "Epoch 41/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.9094 \n",
      "Epoch 41: val_loss improved from 0.22682 to 0.22150, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.2600 - accuracy: 0.9094 - val_loss: 0.2215 - val_accuracy: 0.9261\n",
      "Epoch 42/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2680 - accuracy: 0.9083  \n",
      "Epoch 42: val_loss did not improve from 0.22150\n",
      "35/35 [==============================] - 66s 2s/step - loss: 0.2680 - accuracy: 0.9083 - val_loss: 0.2287 - val_accuracy: 0.9234\n",
      "Epoch 43/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2684 - accuracy: 0.9054 \n",
      "Epoch 43: val_loss did not improve from 0.22150\n",
      "35/35 [==============================] - 67s 2s/step - loss: 0.2684 - accuracy: 0.9054 - val_loss: 0.2221 - val_accuracy: 0.9243\n",
      "Epoch 44/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2713 - accuracy: 0.9054 \n",
      "Epoch 44: val_loss improved from 0.22150 to 0.21387, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 68s 2s/step - loss: 0.2713 - accuracy: 0.9054 - val_loss: 0.2139 - val_accuracy: 0.9333\n",
      "Epoch 45/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2682 - accuracy: 0.9029 \n",
      "Epoch 45: val_loss improved from 0.21387 to 0.21268, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.2682 - accuracy: 0.9029 - val_loss: 0.2127 - val_accuracy: 0.9324\n",
      "Epoch 46/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2676 - accuracy: 0.9022 \n",
      "Epoch 46: val_loss improved from 0.21268 to 0.20934, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2676 - accuracy: 0.9022 - val_loss: 0.2093 - val_accuracy: 0.9360\n",
      "Epoch 47/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.9085  \n",
      "Epoch 47: val_loss improved from 0.20934 to 0.20763, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2647 - accuracy: 0.9085 - val_loss: 0.2076 - val_accuracy: 0.9342\n",
      "Epoch 48/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.9092 \n",
      "Epoch 48: val_loss improved from 0.20763 to 0.20527, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 84s 2s/step - loss: 0.2471 - accuracy: 0.9092 - val_loss: 0.2053 - val_accuracy: 0.9387\n",
      "Epoch 49/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.9153  \n",
      "Epoch 49: val_loss improved from 0.20527 to 0.20422, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 69s 2s/step - loss: 0.2535 - accuracy: 0.9153 - val_loss: 0.2042 - val_accuracy: 0.9396\n",
      "Epoch 50/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.9182 \n",
      "Epoch 50: val_loss improved from 0.20422 to 0.20055, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 68s 2s/step - loss: 0.2514 - accuracy: 0.9182 - val_loss: 0.2005 - val_accuracy: 0.9369\n",
      "Epoch 51/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9101 \n",
      "Epoch 51: val_loss did not improve from 0.20055\n",
      "35/35 [==============================] - 65s 2s/step - loss: 0.2543 - accuracy: 0.9101 - val_loss: 0.2050 - val_accuracy: 0.9342\n",
      "Epoch 52/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2517 - accuracy: 0.9090 \n",
      "Epoch 52: val_loss improved from 0.20055 to 0.19839, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.2517 - accuracy: 0.9090 - val_loss: 0.1984 - val_accuracy: 0.9387\n",
      "Epoch 53/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.9182  \n",
      "Epoch 53: val_loss improved from 0.19839 to 0.19640, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 70s 2s/step - loss: 0.2433 - accuracy: 0.9182 - val_loss: 0.1964 - val_accuracy: 0.9405\n",
      "Epoch 54/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2587 - accuracy: 0.9110  \n",
      "Epoch 54: val_loss did not improve from 0.19640\n",
      "35/35 [==============================] - 67s 2s/step - loss: 0.2587 - accuracy: 0.9110 - val_loss: 0.2010 - val_accuracy: 0.9333\n",
      "Epoch 55/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2618 - accuracy: 0.9074 \n",
      "Epoch 55: val_loss did not improve from 0.19640\n",
      "35/35 [==============================] - 68s 2s/step - loss: 0.2618 - accuracy: 0.9074 - val_loss: 0.2055 - val_accuracy: 0.9279\n",
      "Epoch 56/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2476 - accuracy: 0.9123 \n",
      "Epoch 56: val_loss did not improve from 0.19640\n",
      "35/35 [==============================] - 67s 2s/step - loss: 0.2476 - accuracy: 0.9123 - val_loss: 0.1997 - val_accuracy: 0.9315\n",
      "Epoch 57/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2385 - accuracy: 0.9130 \n",
      "Epoch 57: val_loss improved from 0.19640 to 0.19273, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 86s 2s/step - loss: 0.2385 - accuracy: 0.9130 - val_loss: 0.1927 - val_accuracy: 0.9387\n",
      "Epoch 58/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.9171  \n",
      "Epoch 58: val_loss improved from 0.19273 to 0.19103, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 70s 2s/step - loss: 0.2467 - accuracy: 0.9171 - val_loss: 0.1910 - val_accuracy: 0.9405\n",
      "Epoch 59/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.9130 \n",
      "Epoch 59: val_loss improved from 0.19103 to 0.18839, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 68s 2s/step - loss: 0.2454 - accuracy: 0.9130 - val_loss: 0.1884 - val_accuracy: 0.9396\n",
      "Epoch 60/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2568 - accuracy: 0.9081 \n",
      "Epoch 60: val_loss did not improve from 0.18839\n",
      "35/35 [==============================] - 66s 2s/step - loss: 0.2568 - accuracy: 0.9081 - val_loss: 0.1993 - val_accuracy: 0.9315\n",
      "Epoch 61/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2379 - accuracy: 0.9164  \n",
      "Epoch 61: val_loss did not improve from 0.18839\n",
      "35/35 [==============================] - 85s 2s/step - loss: 0.2379 - accuracy: 0.9164 - val_loss: 0.1932 - val_accuracy: 0.9342\n",
      "Epoch 62/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2326 - accuracy: 0.9148  \n",
      "Epoch 62: val_loss improved from 0.18839 to 0.18790, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 111s 3s/step - loss: 0.2326 - accuracy: 0.9148 - val_loss: 0.1879 - val_accuracy: 0.9405\n",
      "Epoch 63/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.9072  \n",
      "Epoch 63: val_loss did not improve from 0.18790\n",
      "35/35 [==============================] - 101s 3s/step - loss: 0.2426 - accuracy: 0.9072 - val_loss: 0.1914 - val_accuracy: 0.9342\n",
      "Epoch 64/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2457 - accuracy: 0.9153  \n",
      "Epoch 64: val_loss did not improve from 0.18790\n",
      "35/35 [==============================] - 101s 3s/step - loss: 0.2457 - accuracy: 0.9153 - val_loss: 0.1886 - val_accuracy: 0.9369\n",
      "Epoch 65/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2394 - accuracy: 0.9160  \n",
      "Epoch 65: val_loss improved from 0.18790 to 0.18406, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 108s 3s/step - loss: 0.2394 - accuracy: 0.9160 - val_loss: 0.1841 - val_accuracy: 0.9423\n",
      "Epoch 66/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2333 - accuracy: 0.9153  \n",
      "Epoch 66: val_loss did not improve from 0.18406\n",
      "35/35 [==============================] - 99s 3s/step - loss: 0.2333 - accuracy: 0.9153 - val_loss: 0.1861 - val_accuracy: 0.9387\n",
      "Epoch 67/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2375 - accuracy: 0.9108  \n",
      "Epoch 67: val_loss improved from 0.18406 to 0.18067, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 104s 3s/step - loss: 0.2375 - accuracy: 0.9108 - val_loss: 0.1807 - val_accuracy: 0.9459\n",
      "Epoch 68/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2283 - accuracy: 0.9209  \n",
      "Epoch 68: val_loss did not improve from 0.18067\n",
      "35/35 [==============================] - 100s 3s/step - loss: 0.2283 - accuracy: 0.9209 - val_loss: 0.1831 - val_accuracy: 0.9387\n",
      "Epoch 69/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2318 - accuracy: 0.9132  \n",
      "Epoch 69: val_loss did not improve from 0.18067\n",
      "35/35 [==============================] - 98s 3s/step - loss: 0.2318 - accuracy: 0.9132 - val_loss: 0.1808 - val_accuracy: 0.9423\n",
      "Epoch 70/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2328 - accuracy: 0.9164  \n",
      "Epoch 70: val_loss did not improve from 0.18067\n",
      "35/35 [==============================] - 66s 2s/step - loss: 0.2328 - accuracy: 0.9164 - val_loss: 0.1883 - val_accuracy: 0.9342\n",
      "Epoch 71/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2354 - accuracy: 0.9119 \n",
      "Epoch 71: val_loss improved from 0.18067 to 0.17697, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 65s 2s/step - loss: 0.2354 - accuracy: 0.9119 - val_loss: 0.1770 - val_accuracy: 0.9450\n",
      "Epoch 72/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2245 - accuracy: 0.9169 \n",
      "Epoch 72: val_loss improved from 0.17697 to 0.17652, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.2245 - accuracy: 0.9169 - val_loss: 0.1765 - val_accuracy: 0.9459\n",
      "Epoch 73/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2272 - accuracy: 0.9214 \n",
      "Epoch 73: val_loss did not improve from 0.17652\n",
      "35/35 [==============================] - 65s 2s/step - loss: 0.2272 - accuracy: 0.9214 - val_loss: 0.1767 - val_accuracy: 0.9423\n",
      "Epoch 74/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2310 - accuracy: 0.9153 \n",
      "Epoch 74: val_loss improved from 0.17652 to 0.17494, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 92s 3s/step - loss: 0.2310 - accuracy: 0.9153 - val_loss: 0.1749 - val_accuracy: 0.9459\n",
      "Epoch 75/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2268 - accuracy: 0.9146 \n",
      "Epoch 75: val_loss did not improve from 0.17494\n",
      "35/35 [==============================] - 84s 2s/step - loss: 0.2268 - accuracy: 0.9146 - val_loss: 0.1758 - val_accuracy: 0.9450\n",
      "Epoch 76/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2306 - accuracy: 0.9157 \n",
      "Epoch 76: val_loss improved from 0.17494 to 0.17313, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 66s 2s/step - loss: 0.2306 - accuracy: 0.9157 - val_loss: 0.1731 - val_accuracy: 0.9450\n",
      "Epoch 77/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2299 - accuracy: 0.9200 \n",
      "Epoch 77: val_loss did not improve from 0.17313\n",
      "35/35 [==============================] - 64s 2s/step - loss: 0.2299 - accuracy: 0.9200 - val_loss: 0.1829 - val_accuracy: 0.9360\n",
      "Epoch 78/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.9166 \n",
      "Epoch 78: val_loss did not improve from 0.17313\n",
      "35/35 [==============================] - 63s 2s/step - loss: 0.2254 - accuracy: 0.9166 - val_loss: 0.1744 - val_accuracy: 0.9432\n",
      "Epoch 79/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2232 - accuracy: 0.9171 \n",
      "Epoch 79: val_loss did not improve from 0.17313\n",
      "35/35 [==============================] - 63s 2s/step - loss: 0.2232 - accuracy: 0.9171 - val_loss: 0.1797 - val_accuracy: 0.9369\n",
      "Epoch 80/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2262 - accuracy: 0.9173 \n",
      "Epoch 80: val_loss improved from 0.17313 to 0.17081, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 66s 2s/step - loss: 0.2262 - accuracy: 0.9173 - val_loss: 0.1708 - val_accuracy: 0.9459\n",
      "Epoch 81/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2228 - accuracy: 0.9180 \n",
      "Epoch 81: val_loss did not improve from 0.17081\n",
      "35/35 [==============================] - 63s 2s/step - loss: 0.2228 - accuracy: 0.9180 - val_loss: 0.1748 - val_accuracy: 0.9405\n",
      "Epoch 82/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2283 - accuracy: 0.9153 \n",
      "Epoch 82: val_loss improved from 0.17081 to 0.16906, saving model to ./checkpoints/XLM-RoBERTa__lr5e-05_ep150_bs128.h5\n",
      "35/35 [==============================] - 66s 2s/step - loss: 0.2283 - accuracy: 0.9153 - val_loss: 0.1691 - val_accuracy: 0.9459\n",
      "Epoch 83/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2138 - accuracy: 0.9207 \n",
      "Epoch 83: val_loss did not improve from 0.16906\n",
      "35/35 [==============================] - 63s 2s/step - loss: 0.2138 - accuracy: 0.9207 - val_loss: 0.1713 - val_accuracy: 0.9459\n",
      "Epoch 84/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2243 - accuracy: 0.9209 \n",
      "Epoch 84: val_loss did not improve from 0.16906\n",
      "35/35 [==============================] - 64s 2s/step - loss: 0.2243 - accuracy: 0.9209 - val_loss: 0.1744 - val_accuracy: 0.9405\n",
      "Epoch 85/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2257 - accuracy: 0.9151 \n",
      "Epoch 85: val_loss did not improve from 0.16906\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.2257 - accuracy: 0.9151 - val_loss: 0.1699 - val_accuracy: 0.9459\n",
      "Epoch 86/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2211 - accuracy: 0.9155  \n",
      "Epoch 86: val_loss did not improve from 0.16906\n",
      "35/35 [==============================] - 98s 3s/step - loss: 0.2211 - accuracy: 0.9155 - val_loss: 0.1701 - val_accuracy: 0.9450\n",
      "Epoch 87/150\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2245 - accuracy: 0.9241 \n",
      "Epoch 87: val_loss did not improve from 0.16906\n",
      "35/35 [==============================] - 65s 2s/step - loss: 0.2245 - accuracy: 0.9241 - val_loss: 0.1772 - val_accuracy: 0.9360\n",
      "9/9 [==============================] - 29s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 00:57:04.009270: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 26311934319612686\n",
      "2025-04-20 00:57:04.009542: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 7164406340783375475\n",
      "2025-04-20 00:57:04.009573: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 11318449908913587661\n",
      "2025-04-20 00:57:04.009577: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 8338219912533161115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||-------------------------------------------------------||\n",
      "||--> Run XLM-RoBERTa__lr5e-05_ep150_bs128 evaluation metrics output:\n",
      "Accuracy: 0.9459, Precision: 0.9520, Recall: 0.9419, F1-Score: 0.9469\n",
      "||-------------------------------------------------------||\n",
      "Confusion Matrix:\n",
      "[[515  27]\n",
      " [ 33 535]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import gc\n",
    "import mlflow\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "\n",
    "import argparse\n",
    "\n",
    "MAX_LENGTH = 150\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "randnum = 10#42\n",
    "\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Load dataset\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[['text', 'label']].dropna()\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    return df\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_data(texts, tokenizer):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "# Prepare datasets\n",
    "def prepare_datasets(train_df, val_df, test_df, tokenizer):\n",
    "    train_encodings = tokenize_data(train_df['text'].tolist(), tokenizer)\n",
    "    val_encodings = tokenize_data(val_df['text'].tolist(), tokenizer)\n",
    "    test_encodings = tokenize_data(test_df['text'].tolist(), tokenizer)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'input_ids': train_encodings['input_ids'],\n",
    "            'attention_mask': train_encodings['attention_mask']\n",
    "        },\n",
    "        train_df['label'].values\n",
    "    )).shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'input_ids': val_encodings['input_ids'],\n",
    "            'attention_mask': val_encodings['attention_mask']\n",
    "        },\n",
    "        val_df['label'].values\n",
    "    )).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'input_ids': test_encodings['input_ids'],\n",
    "            'attention_mask': test_encodings['attention_mask']\n",
    "        },\n",
    "        test_df['label'].values\n",
    "    )).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def model_abbre(model_name):\n",
    "    cases = {\n",
    "        \"bert-base-uncased\": \"bert-base\",\n",
    "        \"bert-base-multilingual-cased\": \"mBERT\",\n",
    "        'xlm-roberta-base': 'XLM-RoBERTa' \n",
    "        #'google-bert/bert-base-cased': 'mobileBert'\n",
    "    }\n",
    "    return cases.get(model_name, \"Model Unavailable\")\n",
    "    \n",
    "def run_training(hp, model_name):\n",
    "    mlflow.set_experiment(\"Second Evaluation\")\n",
    "    run_name = f\"{hp[\"exp_desc\"]}_{model_abbre(model_name)}__lr{hp['learning_rate']}_ep{hp['epochs']}_bs{hp['batch_size']}\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.log_params(hp)\n",
    "        mlflow.set_tag(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        print(\"||--------------------------------------||\")        \n",
    "        print(f\"||===>> Starting run: {run_name} with hyperparameters: {hp}\")\n",
    "        print(\"||--------------------------------------||\")        \n",
    "\n",
    "        #Red Info Logs Killer\n",
    "        os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "        \n",
    "        # Print the TensorFlow version\n",
    "        print(f\"TensorFlow version: {tf.__version__}\")\n",
    "        \n",
    "        # List available GPU devices\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            print(\"GPUs detected:\")\n",
    "            for gpu in gpus:\n",
    "                print(gpu)\n",
    "        else:\n",
    "            print(\"No GPUs detected.\")\n",
    "            \n",
    "        if gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        \n",
    "            # Configuration\n",
    "        MODEL_NAME = model_name # also for tokenizer\n",
    "                                 # 'bert-base-uncased' (bert)\n",
    "                                 # 'bert-base-multilingual-cased' (mBERT)\n",
    "                                 # 'xlm-roberta-base' or \"distilroberta-base\" (XLM-RoBERTa, Distil Roberta)\n",
    "                                 # \"google-bert/bert-base-cased\" (mobileBert)\n",
    "\n",
    "        random.seed(randnum)\n",
    "        tf.random.set_seed(randnum)\n",
    "        np.random.seed(randnum)\n",
    "        \n",
    "        model_output_name = \"mbert_logging_test1\"\n",
    "\n",
    "\n",
    "        # Dataset split\n",
    "        df = load_data('dataset/finaldataset_6k_shuffled_v2.csv')\n",
    "        train_df, test_df = train_test_split(df, test_size=0.1, random_state=randnum)\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=randnum)\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "        train_dataset, val_dataset, test_dataset = prepare_datasets(train_df, val_df, test_df, tokenizer)\n",
    "\n",
    "        # Model initialization\n",
    "        model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=2,\n",
    "            # hidden_dropout_prob=0.3,\n",
    "            # attention_probs_dropout_prob=0.15\n",
    "        )\n",
    "        \n",
    "        # Freeze all layers\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "        # Unfreeze classifier layer\n",
    "        model.layers[-1].trainable = True\n",
    "        \n",
    "        # Compile the model\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=hp[\"learning_rate\"])\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "    \n",
    "        # Prepare datasets\n",
    "        train_ds = train_dataset.unbatch().batch(hp[\"batch_size\"])\n",
    "        val_ds = val_dataset.unbatch().batch(hp[\"batch_size\"])\n",
    "\n",
    "        # Prepare callbacks: EarlyStopping and ModelCheckpoint\n",
    "        checkpoint_filepath = f\"./checkpoints/{run_name}.h5\"\n",
    "        os.makedirs(os.path.dirname(checkpoint_filepath), exist_ok=True)\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "            tf.keras.callbacks.TensorBoard(log_dir='logs'),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=checkpoint_filepath,\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: [\n",
    "                mlflow.log_metric(\"train_loss\", logs[\"loss\"], step=epoch),\n",
    "                mlflow.log_metric(\"train_accuracy\", logs[\"accuracy\"], step=epoch),\n",
    "                mlflow.log_metric(\"val_loss\", logs[\"val_loss\"], step=epoch),\n",
    "                mlflow.log_metric(\"val_accuracy\", logs[\"val_accuracy\"], step=epoch),\n",
    "            ])\n",
    "        ]\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=hp[\"epochs\"],\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "    \n",
    "        # Optionally load best checkpoint\n",
    "        if os.path.exists(checkpoint_filepath):\n",
    "            model.load_weights(checkpoint_filepath)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        val_preds = model.predict(val_ds).logits\n",
    "        y_pred = np.argmax(val_preds, axis=1)\n",
    "        y_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "        \n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred)\n",
    "        rec = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        print(\"||-------------------------------------------------------||\")\n",
    "        print(f\"||--> Run {run_name} evaluation metrics output:\")\n",
    "        print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}\")\n",
    "        print(\"||-------------------------------------------------------||\")\n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metrics({\n",
    "            \"val_accuracy\": acc,\n",
    "            \"val_precision\": prec,\n",
    "            \"val_recall\": rec,\n",
    "            \"val_f1_score\": f1\n",
    "        })\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "        plt.xlabel(\"Predicted Labels\")\n",
    "        plt.ylabel(\"True Labels\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "        # Clean up GPU memory\n",
    "        del model\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--learning_rate\", type=float, required=True)\n",
    "    parser.add_argument(\"--epochs\", type=int, required=True)\n",
    "    parser.add_argument(\"--batch_size\", type=int, required=True)\n",
    "    parser.add_argument(\"--model\", type=str, required=True)\n",
    "    parser.add_argument(\"--exp_desc\", type=str, required=False, default=\"oo\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(f\"Learning Rate: {args.learning_rate}, Epochs: {args.epochs}, Batch Size: {args.batch_size} Model: {args.model} Exp Des: {args.exp_desc}\")\n",
    "\n",
    "    # Model Names: \n",
    "    # 'bert-base-uncased' (bert)\n",
    "    # 'bert-base-multilingual-cased' (mBERT)\n",
    "    # 'xlm-roberta-base' or \"distilroberta-base\" (XLM-RoBERTa, Distil Roberta)\n",
    "    # \"google-bert/bert-base-cased\" (mobileBert)\n",
    "\n",
    "    # Prepare hyperparameter dictionary\n",
    "    hyperparams = {\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"epochs\": args.epochs,\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"exp_desc\" : args.exp_desc\n",
    "    }\n",
    "    \n",
    "    run_training(hyperparams, args.model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
