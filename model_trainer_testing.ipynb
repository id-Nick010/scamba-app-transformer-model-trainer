{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "153a0114-94c8-4604-a028-86a7bb722609",
   "metadata": {},
   "source": [
    "# Model Training (TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e56dc83-85ce-4975-802b-89fbb0ec0273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for adding features to the auto model trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430374f4-d2f7-493c-8da6-f8abaace70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "    \n",
    "default_args = {\n",
    "    'learning_rate' : 5e-5,\n",
    "    'epochs' : 150,\n",
    "    'batch_size' : 128,\n",
    "    'model' : 'xlm-roberta-base',\n",
    "    'exp_desc' : 'default_run'\n",
    "}\n",
    "\n",
    "sys.argv = [\"script_name\", \"--learning_rate\", str(default_args[\"learning_rate\"]) ,\"--epochs\", str(default_args[\"epochs\"]),\"--batch_size\", str(default_args[\"batch_size\"]),\"--model\" , default_args[\"model\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1826fe2-0afa-4e56-a2e7-85adc4eb6bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def confusion_matrix(model, test_data):\n",
    "    \n",
    "    # Assuming you have a trained model and test dataset\n",
    "    predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "\n",
    "    true_labels = test_labels  # Ground truth labels\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908ef555-154d-491c-bd39-d7befd43d258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import gc\n",
    "import mlflow\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "\n",
    "import argparse\n",
    "\n",
    "MAX_LENGTH = 150\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "\n",
    "randnum = 10#42\n",
    "\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Load dataset\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[['text', 'label']].dropna()\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    return df\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_data(texts, tokenizer):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "# Prepare datasets\n",
    "def prepare_datasets(train_df, val_df, test_df, tokenizer):\n",
    "    train_encodings = tokenize_data(train_df['text'].tolist(), tokenizer)\n",
    "    val_encodings = tokenize_data(val_df['text'].tolist(), tokenizer)\n",
    "    test_encodings = tokenize_data(test_df['text'].tolist(), tokenizer)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'input_ids': train_encodings['input_ids'],\n",
    "            'attention_mask': train_encodings['attention_mask']\n",
    "        },\n",
    "        train_df['label'].values\n",
    "    )).shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'input_ids': val_encodings['input_ids'],\n",
    "            'attention_mask': val_encodings['attention_mask']\n",
    "        },\n",
    "        val_df['label'].values\n",
    "    )).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'input_ids': test_encodings['input_ids'],\n",
    "            'attention_mask': test_encodings['attention_mask']\n",
    "        },\n",
    "        test_df['label'].values\n",
    "    )).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def model_abbre(model_name):\n",
    "    cases = {\n",
    "        \"bert-base-uncased\": \"bert-base\",\n",
    "        \"bert-base-multilingual-cased\": \"mBERT\",\n",
    "        'xlm-roberta-base': 'XLM-RoBERTa' \n",
    "        #'google-bert/bert-base-cased': 'mobileBert'\n",
    "    }\n",
    "    return cases.get(model_name, \"Model Unavailable\")\n",
    "    \n",
    "def run_training(hp, model_name):\n",
    "    mlflow.set_experiment(\"Second Evaluation\")\n",
    "    run_name = f\"{model_abbre(model_name)}__lr{hp['learning_rate']}_ep{hp['epochs']}_bs{hp['batch_size']}\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.log_params(hp)\n",
    "        mlflow.set_tag(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        print(\"||--------------------------------------||\")        \n",
    "        print(f\"||===>> Starting run: {run_name} with hyperparameters: {hp}\")\n",
    "        print(\"||--------------------------------------||\")        \n",
    "\n",
    "        #Red Info Logs Killer\n",
    "        os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "        \n",
    "        # Print the TensorFlow version\n",
    "        print(f\"TensorFlow version: {tf.__version__}\")\n",
    "        \n",
    "        # List available GPU devices\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            print(\"GPUs detected:\")\n",
    "            for gpu in gpus:\n",
    "                print(gpu)\n",
    "        else:\n",
    "            print(\"No GPUs detected.\")\n",
    "            \n",
    "        if gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        \n",
    "            # Configuration\n",
    "        MODEL_NAME = model_name # also for tokenizer\n",
    "                                 # 'bert-base-uncased' (bert)\n",
    "                                 # 'bert-base-multilingual-cased' (mBERT)\n",
    "                                 # 'xlm-roberta-base' or \"distilroberta-base\" (XLM-RoBERTa, Distil Roberta)\n",
    "                                 # \"google-bert/bert-base-cased\" (mobileBert)\n",
    "\n",
    "        random.seed(randnum)\n",
    "        tf.random.set_seed(randnum)\n",
    "        np.random.seed(randnum)\n",
    "        \n",
    "        model_output_name = \"mbert_logging_test1\"\n",
    "\n",
    "\n",
    "        # Dataset split\n",
    "        df = load_data('dataset/finaldataset_6k_shuffled_v2.csv')\n",
    "        train_df, test_df = train_test_split(df, test_size=0.1, random_state=randnum)\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=randnum)\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "        train_dataset, val_dataset, test_dataset = prepare_datasets(train_df, val_df, test_df, tokenizer)\n",
    "\n",
    "        # Model initialization\n",
    "        model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=2,\n",
    "            # hidden_dropout_prob=0.3,\n",
    "            # attention_probs_dropout_prob=0.15\n",
    "        )\n",
    "        \n",
    "        # Freeze all layers\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "        # Unfreeze classifier layer\n",
    "        model.layers[-1].trainable = True\n",
    "        \n",
    "        # Compile the model\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=hp[\"learning_rate\"])\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "    \n",
    "        # Prepare datasets\n",
    "        train_ds = train_dataset.unbatch().batch(hp[\"batch_size\"])\n",
    "        val_ds = val_dataset.unbatch().batch(hp[\"batch_size\"])\n",
    "\n",
    "        # Prepare callbacks: EarlyStopping and ModelCheckpoint\n",
    "        checkpoint_filepath = f\"./checkpoints/{run_name}.h5\"\n",
    "        os.makedirs(os.path.dirname(checkpoint_filepath), exist_ok=True)\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "            tf.keras.callbacks.TensorBoard(log_dir='logs'),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=checkpoint_filepath,\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: [\n",
    "                mlflow.log_metric(\"train_loss\", logs[\"loss\"], step=epoch),\n",
    "                mlflow.log_metric(\"train_accuracy\", logs[\"accuracy\"], step=epoch),\n",
    "                mlflow.log_metric(\"val_loss\", logs[\"val_loss\"], step=epoch),\n",
    "                mlflow.log_metric(\"val_accuracy\", logs[\"val_accuracy\"], step=epoch),\n",
    "            ])\n",
    "        ]\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=hp[\"epochs\"],\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "    \n",
    "        # Optionally load best checkpoint\n",
    "        if os.path.exists(checkpoint_filepath):\n",
    "            model.load_weights(checkpoint_filepath)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        val_preds = model.predict(val_ds).logits\n",
    "        y_pred = np.argmax(val_preds, axis=1)\n",
    "        y_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "        \n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred)\n",
    "        rec = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        print(\"||-------------------------------------------------------||\")\n",
    "        print(f\"||--> Run {run_name} evaluation metrics output:\")\n",
    "        print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}\")\n",
    "        print(\"||-------------------------------------------------------||\")\n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metrics({\n",
    "            \"val_accuracy\": acc,\n",
    "            \"val_precision\": prec,\n",
    "            \"val_recall\": rec,\n",
    "            \"val_f1_score\": f1\n",
    "        })\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "        plt.xlabel(\"Predicted Labels\")\n",
    "        plt.ylabel(\"True Labels\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "        # Clean up GPU memory\n",
    "        del model\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--learning_rate\", type=float, required=True)\n",
    "    parser.add_argument(\"--epochs\", type=int, required=True)\n",
    "    parser.add_argument(\"--batch_size\", type=int, required=True)\n",
    "    parser.add_argument(\"--model\", type=str, required=True)\n",
    "    parser.add_argument(\"--exp_desc\", type=str, required=False)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(f\"Learning Rate: {args.learning_rate}, Epochs: {args.epochs}, Batch Size: {args.batch_size} Model: {args.model} Exp Des: {args.exp_desc}\")\n",
    "\n",
    "    # Model Names: \n",
    "    # 'bert-base-uncased' (bert)\n",
    "    # 'bert-base-multilingual-cased' (mBERT)\n",
    "    # 'xlm-roberta-base' or \"distilroberta-base\" (XLM-RoBERTa, Distil Roberta)\n",
    "    # \"google-bert/bert-base-cased\" (mobileBert)\n",
    "\n",
    "    # Prepare hyperparameter dictionary\n",
    "    hyperparams = {\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"epochs\": args.epochs,\n",
    "        \"batch_size\": args.batch_size\n",
    "    }\n",
    "    \n",
    "    run_training(hyperparams, args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a344e54-f88b-4a8d-bfad-641040133cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
