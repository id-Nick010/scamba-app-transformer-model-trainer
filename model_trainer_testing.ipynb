{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "153a0114-94c8-4604-a028-86a7bb722609",
   "metadata": {},
   "source": [
    "# Model Training (TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e56dc83-85ce-4975-802b-89fbb0ec0273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for adding features to the auto model trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ca8ff-9738-40a7-96c1-efa8ac9a7341",
   "metadata": {},
   "outputs": [],
   "source": [
    "--learning_rate 3e-5 --epochs 10 --batch_size 32 \\\n",
    "--hidden_dropout_prob 0.3 --attention_probs_dropout_prob 0.15 \\\n",
    "--classifier_dropout 0.3 --extra_dropout 0.2 --l2_strength 1e-5 \\\n",
    "--weight_decay 1e-5\n",
    "\n",
    "    # Model Names: \n",
    "    # 'bert-base-uncased' (bert)\n",
    "    # 'bert-base-multilingual-cased' (mBERT)\n",
    "    # 'xlm-roberta-base' or \"distilroberta-base\" (XLM-RoBERTa, Distil Roberta)\n",
    "    # \"google-bert/bert-base-cased\" (mobileBert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430374f4-d2f7-493c-8da6-f8abaace70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "    \n",
    "default_args = {\n",
    "    'learning_rate' : 5e-5,\n",
    "    'epochs' : 125,\n",
    "    'batch_size' : 16,\n",
    "    'model' : 'bert-base-multilingual-cased',\n",
    "    'exp_desc' : 'test_run'\n",
    "}\n",
    "\n",
    "sys.argv = [\"script_name\", \"--learning_rate\", str(default_args[\"learning_rate\"]) ,\"--epochs\", str(default_args[\"epochs\"]),\"--batch_size\", str(default_args[\"batch_size\"]),\"--model\" , default_args[\"model\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908ef555-154d-491c-bd39-d7befd43d258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import gc\n",
    "import mlflow\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "\n",
    "import argparse\n",
    "\n",
    "MAX_LENGTH = 150\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "randnum = 10#42\n",
    "\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Load dataset\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[['text', 'label']].dropna()\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    return df\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_data(texts, tokenizer):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "# Prepare datasets\n",
    "def prepare_datasets(train_df, val_df, test_df, tokenizer):\n",
    "    train_encodings = tokenize_data(train_df['text'].tolist(), tokenizer)\n",
    "    val_encodings = tokenize_data(val_df['text'].tolist(), tokenizer)\n",
    "    test_encodings = tokenize_data(test_df['text'].tolist(), tokenizer)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'input_ids': train_encodings['input_ids'],\n",
    "            'attention_mask': train_encodings['attention_mask']\n",
    "        },\n",
    "        train_df['label'].values\n",
    "    )).shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'input_ids': val_encodings['input_ids'],\n",
    "            'attention_mask': val_encodings['attention_mask']\n",
    "        },\n",
    "        val_df['label'].values\n",
    "    )).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'input_ids': test_encodings['input_ids'],\n",
    "            'attention_mask': test_encodings['attention_mask']\n",
    "        },\n",
    "        test_df['label'].values\n",
    "    )).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def model_abbre(model_name):\n",
    "    cases = {\n",
    "        \"bert-base-uncased\": \"bert-base\",\n",
    "        \"bert-base-multilingual-cased\": \"mBERT\",\n",
    "        'xlm-roberta-base': 'XLM-RoBERTa' \n",
    "        #'google-bert/bert-base-cased': 'mobileBert'\n",
    "    }\n",
    "    return cases.get(model_name, \"Model Unavailable\")\n",
    "    \n",
    "def run_training(hp, model_name):\n",
    "    mlflow.set_experiment(\"Second Evaluation\")\n",
    "    run_name = f\"{hp['exp_desc']}_{model_abbre(model_name)}__lr{hp['learning_rate']}_ep{hp['epochs']}_bs{hp['batch_size']}\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.log_params(hp)\n",
    "        mlflow.set_tag(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        print(\"||--------------------------------------||\")        \n",
    "        print(f\"||===>> Starting run: {run_name} with hyperparameters: {hp}\")\n",
    "        print(\"||--------------------------------------||\")        \n",
    "\n",
    "        #Red Info Logs Killer\n",
    "        os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "        \n",
    "        # Print the TensorFlow version\n",
    "        print(f\"TensorFlow version: {tf.__version__}\")\n",
    "        \n",
    "        # List available GPU devices\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            print(\"GPUs detected:\")\n",
    "            for gpu in gpus:\n",
    "                print(gpu)\n",
    "        else:\n",
    "            print(\"No GPUs detected.\")\n",
    "            \n",
    "        if gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        \n",
    "            # Configuration\n",
    "        MODEL_NAME = model_name # also for tokenizer\n",
    "                                 # 'bert-base-uncased' (bert)\n",
    "                                 # 'bert-base-multilingual-cased' (mBERT)\n",
    "                                 # 'xlm-roberta-base' or \"distilroberta-base\" (XLM-RoBERTa, Distil Roberta)\n",
    "                                 # \"google-bert/bert-base-cased\" (mobileBert)\n",
    "\n",
    "        random.seed(randnum)\n",
    "        tf.random.set_seed(randnum)\n",
    "        np.random.seed(randnum)\n",
    "        \n",
    "        model_output_name = \"mbert_logging_test1\"\n",
    "\n",
    "\n",
    "        # Dataset split\n",
    "        df = load_data('dataset/finaldataset_6k_shuffled_v2.csv')\n",
    "        train_df, test_df = train_test_split(df, test_size=0.1, random_state=randnum)\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=randnum)\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "        train_dataset, val_dataset, test_dataset = prepare_datasets(train_df, val_df, test_df, tokenizer)\n",
    "\n",
    "        # Model initialization\n",
    "        model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=2,\n",
    "            # hidden_dropout_prob=0.3,\n",
    "            # attention_probs_dropout_prob=0.15\n",
    "        )\n",
    "        \n",
    "        # Freeze all layers\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "        # Unfreeze classifier layer\n",
    "        model.layers[-1].trainable = True\n",
    "        \n",
    "        # Compile the model\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=hp[\"learning_rate\"])\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "    \n",
    "        # Prepare datasets\n",
    "        train_ds = train_dataset.unbatch().batch(hp[\"batch_size\"])\n",
    "        val_ds = val_dataset.unbatch().batch(hp[\"batch_size\"])\n",
    "\n",
    "        # Prepare callbacks: EarlyStopping and ModelCheckpoint\n",
    "        checkpoint_filepath = f\"./checkpoints/{run_name}.h5\"\n",
    "        os.makedirs(os.path.dirname(checkpoint_filepath), exist_ok=True)\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "            tf.keras.callbacks.TensorBoard(log_dir='logs'),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=checkpoint_filepath,\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: [\n",
    "                mlflow.log_metric(\"train_loss\", logs[\"loss\"], step=epoch),\n",
    "                mlflow.log_metric(\"train_accuracy\", logs[\"accuracy\"], step=epoch),\n",
    "                mlflow.log_metric(\"val_loss\", logs[\"val_loss\"], step=epoch),\n",
    "                mlflow.log_metric(\"val_accuracy\", logs[\"val_accuracy\"], step=epoch),\n",
    "            ])\n",
    "        ]\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=hp[\"epochs\"],\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "    \n",
    "        # Optionally load best checkpoint\n",
    "        if os.path.exists(checkpoint_filepath):\n",
    "            model.load_weights(checkpoint_filepath)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        val_preds = model.predict(val_ds).logits\n",
    "        y_pred = np.argmax(val_preds, axis=1)\n",
    "        y_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "        \n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred)\n",
    "        rec = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        print(\"||-------------------------------------------------------||\")\n",
    "        print(f\"||--> Run {run_name} evaluation metrics output:\")\n",
    "        print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}\")\n",
    "        print(\"||-------------------------------------------------------||\")\n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metrics({\n",
    "            \"val_accuracy\": acc,\n",
    "            \"val_precision\": prec,\n",
    "            \"val_recall\": rec,\n",
    "            \"val_f1_score\": f1\n",
    "        })\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "        plt.xlabel(\"Predicted Labels\")\n",
    "        plt.ylabel(\"True Labels\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "        # Clean up GPU memory\n",
    "        del model\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--learning_rate\", type=float, required=True)\n",
    "    parser.add_argument(\"--epochs\", type=int, required=True)\n",
    "    parser.add_argument(\"--batch_size\", type=int, required=True)\n",
    "    parser.add_argument(\"--model\", type=str, required=True)\n",
    "    parser.add_argument(\"--exp_desc\", type=str, required=False, default=\"oo\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(f\"Learning Rate: {args.learning_rate}, Epochs: {args.epochs}, Batch Size: {args.batch_size} Model: {args.model} Exp Des: {args.exp_desc}\")\n",
    "\n",
    "    # Model Names: \n",
    "    # 'bert-base-uncased' (bert)\n",
    "    # 'bert-base-multilingual-cased' (mBERT)\n",
    "    # 'xlm-roberta-base' or \"distilroberta-base\" (XLM-RoBERTa, Distil Roberta)\n",
    "    # \"google-bert/bert-base-cased\" (mobileBert)\n",
    "\n",
    "    # Prepare hyperparameter dictionary\n",
    "    hyperparams = {\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"epochs\": args.epochs,\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"exp_desc\" : args.exp_desc\n",
    "    }\n",
    "    \n",
    "    run_training(hyperparams, args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8da6291-5b39-4af3-9a4b-ffc290355259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  177853440 \n",
      "                                                                 \n",
      " dropout_75 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 177854978 (678.46 MB)\n",
      "Trainable params: 1538 (6.01 KB)\n",
      "Non-trainable params: 177853440 (678.46 MB)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 18:04:34.494007: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f3ac6256220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-20 18:04:34.494084: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Ti Laptop GPU, Compute Capability 8.6\n",
      "2025-04-20 18:04:34.503592: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-04-20 18:04:34.524359: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745143474.594137    2842 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/278 [==============================] - 116s 350ms/step - loss: 0.6851 - accuracy: 0.5527 - val_loss: 0.6714 - val_accuracy: 0.5207\n",
      "Epoch 2/125\n",
      "278/278 [==============================] - 84s 301ms/step - loss: 0.6632 - accuracy: 0.6399 - val_loss: 0.6441 - val_accuracy: 0.5829\n",
      "Epoch 3/125\n",
      "278/278 [==============================] - 86s 310ms/step - loss: 0.6442 - accuracy: 0.6789 - val_loss: 0.6306 - val_accuracy: 0.5937\n",
      "Epoch 4/125\n",
      "278/278 [==============================] - 81s 292ms/step - loss: 0.6276 - accuracy: 0.7064 - val_loss: 0.6030 - val_accuracy: 0.7126\n",
      "Epoch 5/125\n",
      "278/278 [==============================] - 82s 294ms/step - loss: 0.6090 - accuracy: 0.7197 - val_loss: 0.5907 - val_accuracy: 0.7099\n",
      "Epoch 6/125\n",
      "278/278 [==============================] - 84s 301ms/step - loss: 0.5955 - accuracy: 0.7321 - val_loss: 0.5720 - val_accuracy: 0.7423\n",
      "Epoch 7/125\n",
      "278/278 [==============================] - 81s 291ms/step - loss: 0.5859 - accuracy: 0.7406 - val_loss: 0.5505 - val_accuracy: 0.7766\n",
      "Epoch 8/125\n",
      "278/278 [==============================] - 86s 310ms/step - loss: 0.5785 - accuracy: 0.7404 - val_loss: 0.5390 - val_accuracy: 0.7811\n",
      "Epoch 9/125\n",
      "278/278 [==============================] - 93s 332ms/step - loss: 0.5634 - accuracy: 0.7600 - val_loss: 0.5277 - val_accuracy: 0.7811\n",
      "Epoch 10/125\n",
      "278/278 [==============================] - 83s 299ms/step - loss: 0.5624 - accuracy: 0.7591 - val_loss: 0.5108 - val_accuracy: 0.8018\n",
      "Epoch 11/125\n",
      "278/278 [==============================] - 81s 293ms/step - loss: 0.5511 - accuracy: 0.7672 - val_loss: 0.5003 - val_accuracy: 0.8063\n",
      "Epoch 12/125\n",
      "278/278 [==============================] - 103s 371ms/step - loss: 0.5400 - accuracy: 0.7720 - val_loss: 0.4896 - val_accuracy: 0.8126\n",
      "Epoch 13/125\n",
      "278/278 [==============================] - 88s 315ms/step - loss: 0.5351 - accuracy: 0.7787 - val_loss: 0.4809 - val_accuracy: 0.8144\n",
      "Epoch 14/125\n",
      "278/278 [==============================] - 101s 364ms/step - loss: 0.5231 - accuracy: 0.7740 - val_loss: 0.4699 - val_accuracy: 0.8243\n",
      "Epoch 15/125\n",
      "278/278 [==============================] - 80s 286ms/step - loss: 0.5234 - accuracy: 0.7729 - val_loss: 0.4739 - val_accuracy: 0.8090\n",
      "Epoch 16/125\n",
      "278/278 [==============================] - 84s 303ms/step - loss: 0.5199 - accuracy: 0.7763 - val_loss: 0.4573 - val_accuracy: 0.8261\n",
      "Epoch 17/125\n",
      "278/278 [==============================] - 89s 320ms/step - loss: 0.5102 - accuracy: 0.7841 - val_loss: 0.4494 - val_accuracy: 0.8342\n",
      "Epoch 18/125\n",
      "278/278 [==============================] - 102s 364ms/step - loss: 0.5169 - accuracy: 0.7722 - val_loss: 0.4434 - val_accuracy: 0.8378\n",
      "Epoch 19/125\n",
      "278/278 [==============================] - 81s 290ms/step - loss: 0.5069 - accuracy: 0.7837 - val_loss: 0.4443 - val_accuracy: 0.8234\n",
      "Epoch 20/125\n",
      "278/278 [==============================] - 82s 295ms/step - loss: 0.5079 - accuracy: 0.7803 - val_loss: 0.4307 - val_accuracy: 0.8495\n",
      "Epoch 21/125\n",
      "278/278 [==============================] - 80s 288ms/step - loss: 0.4981 - accuracy: 0.7862 - val_loss: 0.4312 - val_accuracy: 0.8387\n",
      "Epoch 22/125\n",
      "278/278 [==============================] - 84s 295ms/step - loss: 0.4937 - accuracy: 0.7938 - val_loss: 0.4280 - val_accuracy: 0.8369\n",
      "Epoch 23/125\n",
      "278/278 [==============================] - 93s 335ms/step - loss: 0.4926 - accuracy: 0.7895 - val_loss: 0.4187 - val_accuracy: 0.8477\n",
      "Epoch 24/125\n",
      "278/278 [==============================] - 81s 291ms/step - loss: 0.4912 - accuracy: 0.7785 - val_loss: 0.4150 - val_accuracy: 0.8505\n",
      "Epoch 25/125\n",
      "278/278 [==============================] - 79s 286ms/step - loss: 0.4860 - accuracy: 0.7927 - val_loss: 0.4158 - val_accuracy: 0.8414\n",
      "Epoch 26/125\n",
      "278/278 [==============================] - 84s 301ms/step - loss: 0.4847 - accuracy: 0.7911 - val_loss: 0.4066 - val_accuracy: 0.8477\n",
      "Epoch 27/125\n",
      "278/278 [==============================] - 79s 285ms/step - loss: 0.4877 - accuracy: 0.7859 - val_loss: 0.4067 - val_accuracy: 0.8486\n",
      "Epoch 28/125\n",
      "278/278 [==============================] - 83s 299ms/step - loss: 0.4852 - accuracy: 0.7884 - val_loss: 0.4024 - val_accuracy: 0.8495\n",
      "Epoch 29/125\n",
      "278/278 [==============================] - 101s 363ms/step - loss: 0.4776 - accuracy: 0.7862 - val_loss: 0.4004 - val_accuracy: 0.8514\n",
      "Epoch 30/125\n",
      "278/278 [==============================] - 101s 362ms/step - loss: 0.4683 - accuracy: 0.8008 - val_loss: 0.3983 - val_accuracy: 0.8495\n",
      "Epoch 31/125\n",
      "278/278 [==============================] - 86s 308ms/step - loss: 0.4785 - accuracy: 0.7893 - val_loss: 0.3979 - val_accuracy: 0.8505\n",
      "Epoch 32/125\n",
      "278/278 [==============================] - 87s 313ms/step - loss: 0.4764 - accuracy: 0.7830 - val_loss: 0.3913 - val_accuracy: 0.8523\n",
      "Epoch 33/125\n",
      "278/278 [==============================] - 82s 293ms/step - loss: 0.4738 - accuracy: 0.7868 - val_loss: 0.3894 - val_accuracy: 0.8505\n",
      "Epoch 34/125\n",
      "278/278 [==============================] - 99s 355ms/step - loss: 0.4627 - accuracy: 0.7981 - val_loss: 0.3871 - val_accuracy: 0.8523\n",
      "Epoch 35/125\n",
      "278/278 [==============================] - 82s 293ms/step - loss: 0.4649 - accuracy: 0.7920 - val_loss: 0.3923 - val_accuracy: 0.8486\n",
      "Epoch 36/125\n",
      "278/278 [==============================] - 81s 293ms/step - loss: 0.4709 - accuracy: 0.7934 - val_loss: 0.3840 - val_accuracy: 0.8514\n",
      "Epoch 37/125\n",
      "278/278 [==============================] - 103s 372ms/step - loss: 0.4696 - accuracy: 0.7913 - val_loss: 0.3792 - val_accuracy: 0.8523\n",
      "Epoch 38/125\n",
      "278/278 [==============================] - 80s 289ms/step - loss: 0.4602 - accuracy: 0.7954 - val_loss: 0.3835 - val_accuracy: 0.8523\n",
      "Epoch 39/125\n",
      "278/278 [==============================] - 80s 288ms/step - loss: 0.4595 - accuracy: 0.8010 - val_loss: 0.3792 - val_accuracy: 0.8523\n",
      "Epoch 40/125\n",
      "278/278 [==============================] - 84s 302ms/step - loss: 0.4605 - accuracy: 0.7970 - val_loss: 0.3761 - val_accuracy: 0.8523\n",
      "Epoch 41/125\n",
      "278/278 [==============================] - 80s 288ms/step - loss: 0.4565 - accuracy: 0.7986 - val_loss: 0.3803 - val_accuracy: 0.8486\n",
      "Epoch 42/125\n",
      "278/278 [==============================] - 81s 292ms/step - loss: 0.4578 - accuracy: 0.8004 - val_loss: 0.3796 - val_accuracy: 0.8486\n",
      "Epoch 43/125\n",
      "278/278 [==============================] - 92s 330ms/step - loss: 0.4564 - accuracy: 0.8031 - val_loss: 0.3730 - val_accuracy: 0.8523\n",
      "Epoch 44/125\n",
      "278/278 [==============================] - 88s 317ms/step - loss: 0.4519 - accuracy: 0.7999 - val_loss: 0.3742 - val_accuracy: 0.8514\n",
      "Epoch 45/125\n",
      "278/278 [==============================] - 85s 307ms/step - loss: 0.4553 - accuracy: 0.7970 - val_loss: 0.3684 - val_accuracy: 0.8541\n",
      "Epoch 46/125\n",
      "278/278 [==============================] - 90s 325ms/step - loss: 0.4531 - accuracy: 0.7970 - val_loss: 0.3677 - val_accuracy: 0.8541\n",
      "Epoch 47/125\n",
      "278/278 [==============================] - 101s 363ms/step - loss: 0.4594 - accuracy: 0.7938 - val_loss: 0.3639 - val_accuracy: 0.8550\n",
      "Epoch 48/125\n",
      "278/278 [==============================] - 89s 320ms/step - loss: 0.4475 - accuracy: 0.8024 - val_loss: 0.3672 - val_accuracy: 0.8523\n",
      "Epoch 49/125\n",
      "278/278 [==============================] - 87s 312ms/step - loss: 0.4417 - accuracy: 0.8035 - val_loss: 0.3685 - val_accuracy: 0.8532\n",
      "Epoch 50/125\n",
      "278/278 [==============================] - 91s 326ms/step - loss: 0.4467 - accuracy: 0.8105 - val_loss: 0.3626 - val_accuracy: 0.8550\n",
      "Epoch 51/125\n",
      "278/278 [==============================] - 96s 347ms/step - loss: 0.4533 - accuracy: 0.7938 - val_loss: 0.3566 - val_accuracy: 0.8550\n",
      "Epoch 52/125\n",
      "278/278 [==============================] - 85s 305ms/step - loss: 0.4554 - accuracy: 0.7990 - val_loss: 0.3592 - val_accuracy: 0.8568\n",
      "Epoch 53/125\n",
      "278/278 [==============================] - 89s 319ms/step - loss: 0.4407 - accuracy: 0.8037 - val_loss: 0.3588 - val_accuracy: 0.8559\n",
      "Epoch 54/125\n",
      "278/278 [==============================] - 91s 327ms/step - loss: 0.4522 - accuracy: 0.7970 - val_loss: 0.3560 - val_accuracy: 0.8568\n",
      "Epoch 55/125\n",
      "278/278 [==============================] - 105s 378ms/step - loss: 0.4424 - accuracy: 0.8078 - val_loss: 0.3529 - val_accuracy: 0.8577\n",
      "Epoch 56/125\n",
      "278/278 [==============================] - 90s 323ms/step - loss: 0.4474 - accuracy: 0.8022 - val_loss: 0.3522 - val_accuracy: 0.8595\n",
      "Epoch 57/125\n",
      "278/278 [==============================] - 91s 326ms/step - loss: 0.4376 - accuracy: 0.8053 - val_loss: 0.3530 - val_accuracy: 0.8577\n",
      "Epoch 58/125\n",
      "278/278 [==============================] - 92s 330ms/step - loss: 0.4480 - accuracy: 0.7979 - val_loss: 0.3537 - val_accuracy: 0.8568\n",
      "Epoch 59/125\n",
      "278/278 [==============================] - 93s 335ms/step - loss: 0.4377 - accuracy: 0.8026 - val_loss: 0.3509 - val_accuracy: 0.8577\n",
      "Epoch 60/125\n",
      "278/278 [==============================] - 92s 333ms/step - loss: 0.4432 - accuracy: 0.8015 - val_loss: 0.3465 - val_accuracy: 0.8577\n",
      "Epoch 61/125\n",
      "278/278 [==============================] - 105s 378ms/step - loss: 0.4410 - accuracy: 0.8037 - val_loss: 0.3453 - val_accuracy: 0.8595\n",
      "Epoch 62/125\n",
      "278/278 [==============================] - 88s 317ms/step - loss: 0.4370 - accuracy: 0.8064 - val_loss: 0.3525 - val_accuracy: 0.8577\n",
      "Epoch 63/125\n",
      "278/278 [==============================] - 89s 321ms/step - loss: 0.4464 - accuracy: 0.8015 - val_loss: 0.3458 - val_accuracy: 0.8595\n",
      "Epoch 64/125\n",
      "278/278 [==============================] - 81s 290ms/step - loss: 0.4383 - accuracy: 0.8087 - val_loss: 0.3495 - val_accuracy: 0.8568\n",
      "Epoch 65/125\n",
      "278/278 [==============================] - 80s 287ms/step - loss: 0.4404 - accuracy: 0.7990 - val_loss: 0.3476 - val_accuracy: 0.8595\n",
      "Epoch 66/125\n",
      "278/278 [==============================] - 82s 293ms/step - loss: 0.4364 - accuracy: 0.8071 - val_loss: 0.3470 - val_accuracy: 0.8595\n",
      "70/70 [==============================] - 21s 224ms/step\n",
      "Val → Acc: 0.8595, Prec: 0.8249, Rec: 0.9208, F1: 0.8702\n",
      "Confusion matrix:\n",
      " [[431 111]\n",
      " [ 45 523]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import argparse\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "import mlflow\n",
    "\n",
    "# Suppress oneDNN warnings (optional)\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"  # reduce TF logs\n",
    "\n",
    "# Constants\n",
    "MAX_LENGTH = 150\n",
    "SEED = 10\n",
    "\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "def load_data(fp):\n",
    "    df = pd.read_csv(fp)[['text','label']].dropna()\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    return df\n",
    "\n",
    "def tokenize_data(texts, tokenizer):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "def prepare_datasets(train_df, val_df, test_df, tokenizer, batch_size):\n",
    "    def to_ds(df):\n",
    "        enc = tokenize_data(df['text'].tolist(), tokenizer)\n",
    "        return tf.data.Dataset.from_tensor_slices((\n",
    "            {'input_ids': enc['input_ids'], 'attention_mask': enc['attention_mask']},\n",
    "            df['label'].values\n",
    "        ))\n",
    "    train_ds = to_ds(train_df).shuffle(1000, seed=SEED).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds   = to_ds(val_df).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds  = to_ds(test_df).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "def model_abbre(name):\n",
    "    return {\n",
    "        \"bert-base-uncased\": \"BERT\",\n",
    "        \"bert-base-multilingual-cased\": \"mBERT\",\n",
    "        \"xlm-roberta-base\": \"XLM-R\",\n",
    "    }.get(name, \"Model\")\n",
    "\n",
    "def run_training(hp, model_name):\n",
    "    mlflow.set_experiment(\"Second Evaluation\")\n",
    "    run_name = f\"{hp['exp_desc']}_{model_abbre(model_name)}_lr{hp['learning_rate']}_ep{hp['epochs']}_bs{hp['batch_size']}\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.log_params(hp)\n",
    "        mlflow.set_tag(\"model_name\", model_name)\n",
    "\n",
    "        # Seeds\n",
    "        random.seed(SEED)\n",
    "        np.random.seed(SEED)\n",
    "        tf.random.set_seed(SEED)\n",
    "\n",
    "        # Data split\n",
    "        df = load_data('dataset/finaldataset_6k_shuffled_v2.csv')\n",
    "        train_df, test_df = train_test_split(df, test_size=0.1, random_state=SEED)\n",
    "        train_df, val_df  = train_test_split(train_df, test_size=0.2, random_state=SEED)\n",
    "\n",
    "        # Tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Config + dropout\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            hidden_dropout_prob=0.3,\n",
    "            attention_probs_dropout_prob=0.2,\n",
    "            classifier_dropout=0.3\n",
    "        )\n",
    "        config.num_labels = 2  # <— set num_labels here\n",
    "\n",
    "        model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        # Freeze base, unfreeze classifier\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "        model.classifier.trainable = True\n",
    "\n",
    "        # L2 on classifier head\n",
    "        l2_reg = tf.keras.regularizers.L2(0.005)\n",
    "        model.classifier.kernel_regularizer = l2_reg\n",
    "        model.classifier.bias_regularizer   = l2_reg\n",
    "\n",
    "        # Datasets\n",
    "        train_ds, val_ds, test_ds = prepare_datasets(\n",
    "            train_df, val_df, test_df, tokenizer, hp['batch_size']\n",
    "        )\n",
    "\n",
    "        # Optimizer & loss\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp['learning_rate'],\n",
    "            weight_decay=0.01,\n",
    "            clipnorm=1.0\n",
    "        )\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True,\n",
    "        )\n",
    "\n",
    "        print(model.summary())\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss,\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        # Callbacks\n",
    "        ckpt_fp = f\"./checkpoints/{run_name}.h5\"\n",
    "        os.makedirs(os.path.dirname(ckpt_fp), exist_ok=True)\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=ckpt_fp,\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=True\n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.TensorBoard(log_dir='logs'),\n",
    "            tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda ep, logs: [\n",
    "                mlflow.log_metric(k, logs[k], step=ep) for k in [\"loss\",\"accuracy\",\"val_loss\",\"val_accuracy\"]\n",
    "            ])\n",
    "        ]\n",
    "\n",
    "        # Train\n",
    "        model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=hp['epochs'],\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Load best weights & evaluate\n",
    "        if os.path.exists(ckpt_fp):\n",
    "            model.load_weights(ckpt_fp)\n",
    "\n",
    "        y_true = np.concatenate([y for _, y in val_ds], axis=0)\n",
    "        y_pred = np.argmax(model.predict(val_ds).logits, axis=1)\n",
    "\n",
    "        acc, prec, rec, f1 = (\n",
    "            accuracy_score(y_true, y_pred),\n",
    "            precision_score(y_true, y_pred),\n",
    "            recall_score(y_true, y_pred),\n",
    "            f1_score(y_true, y_pred)\n",
    "        )\n",
    "        print(f\"Val → Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}\")\n",
    "        print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "        # Cleanup\n",
    "        del model\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--learning_rate\", type=float, required=True)\n",
    "    parser.add_argument(\"--epochs\",        type=int,   required=True)\n",
    "    parser.add_argument(\"--batch_size\",    type=int,   required=True)\n",
    "    parser.add_argument(\"--model\",         type=str,   required=True)\n",
    "    parser.add_argument(\"--exp_desc\",      type=str,   default=\"oo\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    hp = {\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"epochs\":        args.epochs,\n",
    "        \"batch_size\":    args.batch_size,\n",
    "        \"exp_desc\":      args.exp_desc\n",
    "    }\n",
    "    run_training(hp, args.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f2be22-44a5-4dac-be9b-5d89b247b445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
