{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e5b07c7-04b6-4f0e-8578-79e8a03f7a77",
   "metadata": {},
   "source": [
    "# Tokenization Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b404d2c-c774-4c30-bd6a-1bd609501ce0",
   "metadata": {},
   "source": [
    "### Testing Tokenizer Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2269633-8bda-4572-b3b7-b615d3525b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Names: \n",
    "# 'bert-base-uncased' (bert)\n",
    "# 'bert-base-multilingual-cased' (mBERT)\n",
    "# 'xlm-roberta-base' or \"distilroberta-base\" (XLM-RoBERTa, Distil Roberta)\n",
    "# \"google-bert/bert-base-cased\" (mobileBert)\n",
    "\n",
    "MODEL_NAME = 'xlm-roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "272489bf-d89f-45fc-9c0a-b9a87e6c5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Cheers to a new year, Scott! Start 2024 with a bang as a Smart Postpaid subscriber! Reap your rewards with a P5,000 Welcome Gift from UnionBank when you apply for their UnionBank Rewards Credit Card and set up Auto Bills Payment for your Postpaid Account. But that's not all - say GOODBYE to Annual Fees FOREVER! Just spend P20,000 within 60 days of approval. Apply now! smrt.ph/UBWelcomeGift Promo runs until Feb. 29, 2024. T&Cs apply. DTI#181104\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fe17488-7f05-41c5-be8b-88538de90462",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db08daed-8b5f-4116-b29f-70499865554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     num_labels=2,\n",
    "#     # hidden_dropout_prob=0.3,\n",
    "#     # attention_probs_dropout_prob=0.15\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c5ea62a-2d62-4e61-821b-84d6a620c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input\n",
    "encoded_input = tokenizer(\n",
    "    text, \n",
    "    max_length=MAX_LENGTH,\n",
    "    truncation=True,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "# forward pass\n",
    "# output = model(**encoded_input)\n",
    "\n",
    "# print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "126604ed-d3ae-42c7-bc49-4623d9254073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 8\n"
     ]
    }
   ],
   "source": [
    "num_tokens = encoded_input[\"input_ids\"].shape[1]\n",
    "print(f\"Number of tokens: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4430990a-7b01-49f5-93b9-8a1449863548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- tokenized text object value:\n",
      "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=\n",
      "array([[    0, 47801,   929,   823,  8699, 41267,   408,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "\n",
      "\n",
      "--- decoded text from tokenized data:\n",
      "Nickol Jairo Belgica\n",
      "--- real text:\n",
      "Nickol Jairo Belgica\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(encoded_input[\"input_ids\"].numpy()[0], skip_special_tokens=True)\n",
    "print(\"--- tokenized text object value:\")\n",
    "print(encoded_input)\n",
    "print(\"\\n\")\n",
    "print(\"--- decoded text from tokenized data:\") \n",
    "print(decoded_text)\n",
    "print(\"--- real text:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b76d7a-1337-4aea-bd40-349bab307b9d",
   "metadata": {},
   "source": [
    "### Show token number > text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0319ba00-f2e3-4e22-960c-57655e16a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"bagay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6ac0ca25-91d2-4c7c-ab5e-00959a1cbd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58316 > â–bagay\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Sample text\n",
    "# text = \"This is an example sentence for RoBERTa tokenization.\"\n",
    "\n",
    "# Tokenize the text and get token IDs\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Format each token with its respective token ID\n",
    "formatted_tokens = \"\\n\".join(f\"{token_id} > {token}\" for token_id, token in zip(token_ids, tokens))\n",
    "\n",
    "# Print the result\n",
    "print(formatted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc174886-8bf7-48b5-9c17-87322f0fa3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
